---
title: 'Psych 251 PS4: Simulation + Analysis'
author: "Mike Frank"
date: "2019"
output: 
  html_document:
    toc: true
---

This is problem set #4, in which we want you to integrate your knowledge of data wrangling with some basic simulation skills. It's a short problem set to help consolidate your `ggplot2` skills and then help you get your feet wet in testing statistical concepts through "making up data" rather than consulting a textbook or doing math. 

For ease of reading, please separate your answers from our text by marking our text with the `>` character (indicating quotes). 

# Part 1: ggplot practice

This part is a warmup, it should be relatively straightforward `ggplot2` practice.

Load data from Frank, Vul, Saxe (2011, Infancy), a study in which we measured infants' looking to hands in moving scenes. There were infants from 3 months all the way to about two years, and there were two movie conditions (`Faces_Medium`, in which kids played on a white background, and `Faces_Plus`, in which the backgrounds were more complex and the people in the videos were both kids and adults). An eye-tracker measured children's attention to faces. This version of the dataset only gives two conditions and only shows the amount of looking at hands (other variables were measured as well). 

```{r, warning=F, message=F}
library(tidyverse)
library(ggthemes)
library(reshape2)
```

```{r}
fvs <- read_csv("data/FVS2011-hands.csv")
```

First, use `ggplot` to plot a histogram of the ages of children in the study. NOTE: this is a repeated measures design, so you can't just take a histogram of every measurement. 

```{r}
histogram_age <- fvs %>% 
  filter(condition == "Faces_Medium") %>% # this filter removes repeated ages 
  ggplot(aes(age)) +
  geom_histogram(binwidth = 1) + # creates histogram and adjusts binwidth appropriately
  scale_x_continuous(breaks = seq(0, 30, by = 5)) + # sets intervals for x-axis tick labels
  coord_cartesian(xlim=c(0, 30)) + # sets start and end points of x-axis
  xlab("Age (in months)") + # adds custom x-axis label
  theme_classic()

print(histogram_age)
```

Second, make a scatter plot showing hand looking as a function of age and condition. Add appropriate smoothing lines. Take the time to fix the axis labels and make the plot look nice.

```{r}
plot_handlooking_byage_condition <- fvs %>% 
  ggplot(aes(x = age, y = hand.look, color = condition)) +
  geom_point() +
  geom_smooth(method = lm) + # adds appropriate smoothing lines
  scale_x_continuous(breaks = seq(0, 30, by = 5)) + # sets intervals for x-axis tick labels
  xlab("Age (in months)") + # adds custom x-axis label
  ylab("Hand Looking") + # adds custom y-axis label
  scale_color_discrete(name = "Condition", labels = c("Faces Medium", "Faces Plus")) + # adds custom labels to legend
  theme_classic()


print(plot_handlooking_byage_condition)
```

What do you conclude from this pattern of data?

> The plot above suggests that, with age, children spent more time looking at hands overall, but this was especially true in the "faces plus" condition, where backgrounds were more complex and both children and adults were featured. Thus, there seems to be an interaction between age and condition, such that, with age, children's looking at hands increases more in the "faces plus" condition than in the "faces medium" condition. 

What statistical analyses would you perform here to quantify these differences?

> To quantify these differences, you could perform either a repeated-measures ANOVA or a linear mixed effects model, with condition entered as a repeated-measures, categorical predictor and age entered as a continous predictor. Such a model would allow for examining not only the main effects of these predictors on the dependent variable (children's hand looking), but also the effect of an interaction between these predictors (age x condition). This would allow for quantifying the difference between the increase in hand looking with age in the "faces plus" condition vs. the "faces medium" condition.


# Part 2: Simulation

```{r, warning=F, message=F}
library(tidyverse)
```

Let's start by convincing ourselves that t-tests have the appropriate false positive rate. Run 10,000 t-tests with standard, normally-distributed data from a made up 30-person, single-measurement experiment (the command for sampling from a normal distribution is `rnorm`).

The goal of these t-tests are to determine, based on 30 observations, whether the underlying distribution (in this case a normal distribution with mean 0 and standard deviation 1) has a mean that is different from 0. In reality, the mean is not different from 0 (we sampled it using `rnorm`), but sometimes the 30 observations we get in our experiment will suggest that the mean is higher or lower. In this case, we'll get a "significant" result and incorrectly reject the null hypothesis of mean 0.

What's the proportion of "significant" results ($p < .05$) that you see?

First do this using a `for` loop.

> Please note that I referenced the following online resource when writing the code below: https://rpubs.com/ndphillips/47112

```{r}
set.seed(123) # pseudo-random number generator that allows for optimal reproducibility of the rest of the code in this report

num_sim <- 10000 # creates a vector with number of t-tests to be run

p_values <- rep(NA, num_sim) # creates a placeholder vector (where the output of the loop will be saved) with 10000 'NA' values

for (i in 1:num_sim) {
  sample_data <- rnorm(30, mean = 0, sd = 1)
  test <- t.test(sample_data)
  current_p_value <- test$p.value
  p_values[i] <- current_p_value
} # for loop that runs 10000 t-tests on samples of n = 30 observations, randomly generated from the normal distribution

p_values_lt05 <- p_values < 0.05 # creates a vector containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

proportion_05 <- sum(p_values_lt05 == TRUE)/10000 # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(proportion_05) # prints the result
```

Next, do this using the `replicate` function:

> Please note that I referenced the following online resource when writing the code below: https://www.stat.berkeley.edu/~s133/Random2a.html

```{r}
p_values_rep <- replicate(10000, t.test(rnorm(30))$p.value) # creates a vector containing the p-values from 10000 t-tests run on samples of n = 30 observations, randomly generated from the normal distribution 

p_values_rep_lt05 <- p_values < 0.05 # creates a vector containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

rep_proportion_05 <- sum(p_values_rep_lt05 == TRUE)/10000 # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(rep_proportion_05) # prints the result
```

How does this compare to the intended false-positive rate of $\alpha=0.05$?

> The answer I generated with each of the two chunks above is 0.0465, which is incredibly close to (and rounds up to) the intended false positive rate of alpha = 0.05.

Ok, that was a bit boring. Let's try something more interesting - let's implement a p-value sniffing simulation, in the style of Simons, Nelson, & Simonsohn (2011).

Consider this scenario: you have done an experiment, again with 30 participants (one observation each, just for simplicity). The question is whether the true mean is different from 0. You aren't going to check the p-value every trial, but let's say you run 30 - then if the p-value is within the range p < .25 and p > .05, you optionally run 30 more and add those data, then test again. But if the original p value is < .05, you call it a day, and if the original is > .25, you also stop.  

First, write a function that implements this sampling regime.

```{r}
double.sample <- function (x) {
  test_first_sample <- t.test(x) # generates t-test from sample specified by 'x'
  p_value_final <- test_first_sample$p.value # extracts p-value from first t-test
  if(p_value_final < .25 & p_value_final > .05){ 
    second_sample <- x + rnorm(30) 
    test_second_sample <- t.test(second_sample)
    p_value_final <- test_second_sample$p.value
  } # if else structure doubles sample if specified conditions are met and extracts new p-value
  return(p_value_final) # returns either p-value from original t-test or t-test on doubled sample
}
```

Now call this function 10k times and find out what happens. 

```{r}
num_sim_v2 <- 10000 # creates a vector with number of t-tests to be run

p_values_v2 <- rep(NA, num_sim_v2) # creates a placeholder vector (where the output of the loop will be saved) with 10000 'NA' values

for (i in 1:num_sim_v2) {
   sample <- rnorm(30)
   p_value_v2 <- double.sample(sample)
   p_values_v2[i] <- p_value_v2
} # for loop that runs "double sample" function created above on 10000 samples of n = 30 observations, randomly generated from the normal distribution

p_values_lt05_v2 <- p_values_v2 < 0.05 # creates a vector containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

proportion_05_v2 <- sum(p_values_lt05_v2 == TRUE)/10000 # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(proportion_05_v2) # prints the result
```

Is there an inflation of false positives? How bad is it?

> Yes, the false positive rate calculated using the "double sample" strategy is 0.0748. Under typical sampling conditions (where the expected false positive rate is alpha = 0.05), a false positive would result for 500 samples out of 10000. Under the double sample strategy, this number increases to 748 out of 10000. 

Now modify this code so that you can investigate this "double the sample" rule in a bit more depth. In the previous question, the researcher doubles the sample only when they think they got "close" to a significant result, i.e. when their not-significant p is less than 0.25. What if the researcher was more optimistic? See what happens in these 3 other scenarios:

* The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.5.
* The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.75.
* The research doubles their sample whenever they get ANY pvalue that is not significant.

How do these choices affect the false positive rate?

HINT: Try to do this by making the function `double.sample` take the upper p value as an argument, so that you can pass this through dplyr.

HINT 2: You may need more samples. Find out by looking at how the results change from run to run.

```{r}

list <- c(.5, .75, 1) # creates list of p-values

double_sample_v2 <- function (x, p) {
  sample_1 <- rnorm(x)
  test_sample_1 <- t.test(sample_1)
  p_value_final <- test_sample_1$p.value
  if(p_value_final < p & p_value_final > .05){
    sample_2 <- append(sample_1, rnorm(x))
    test_sample_2 <- t.test(sample_2)
    p_value_final <- test_sample_2$p.value
    return(p_value_final)
  } else {return(p_value_final)}
} # generates function that doubles the sample if any of the three conditions specified above are met by the initial t-test of N = 30 observations

x <- 30 # sets initial number of observations to be randomly pulled from a normal distribution

p_values_all <- replicate(10000, sapply(list, double_sample_v2, x=x)) # applies the new 'double sample' function to the three p-values in the list created above and puts them into a new matrix

p_values_all_df <- setNames(melt(p_values_all), c("upper_p_value","iteration", "p_value")) # converts matrix to data frame and names columns in this new data frame

p_values_all_df <- p_values_all_df %>% 
  mutate(upper_p_value = as.factor(upper_p_value)) %>% 
  mutate(upper_p_value = ifelse(upper_p_value == '1', "p < .5", 
                            ifelse(upper_p_value == '2', "p < .75",
                                   ifelse(upper_p_value == '3', "p < 1", NA)))) # relabels the three specified p-values

p_values_all_df <- p_values_all_df %>% 
  mutate(significant = ifelse((p_value < .05), "TRUE", "FALSE")) # creates a column containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

proportions <- p_values_all_df %>% 
  group_by(upper_p_value) %>% 
  summarise(proportion_true = sum(significant == "TRUE")/10000) # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(proportions) # prints the result
```


What do you conclude on the basis of this simulation? How bad is this kind of data-dependent policy?

> These simulations clearly demonstrate that optional stopping inflates the false positive rate; this gets even worse as one loosens the 'rules' they use for whether or not to collect more data. For example, deciding to double the sample whenever one does not achieve a significant result (where p < .05) inflates false positives even more than deciding to only double the sample when .05 < p < .25. And the above simulations do not reflect the full extent to which optional stopping could inflate false positive rates. Imagine one still did not achieve a significant p-value after doubling the sample; they could easily double it again and keep adding participants until they had achieved a significant p-value. This would result in even higher false positive rates than the ones shown above. (Another strategy that would increase false positives is checking the data more often -- e.g., checking after every participant rather than after every 30 participants).

> While the code chunks below only account for doubling the sample once, I've also included code that examines what would happen if the sample were doubled repeatedly until the result was a p-value less than .05 or greater than .25.

```{r, eval = FALSE}
double_sample_loop_one_p <- function (x) {
  sample_1 <- rnorm(x)
  test_sample_1 <- t.test(sample_1)
  p_value_final <- test_sample_1$p.value
  while (p_value_final < .25 & p_value_final > .05){
    sample_2 <- append(sample_1, rnorm(x))
    test_sample_2 <- t.test(sample_2)
    p_value_final <- test_sample_2$p.value
    sample_1 <- sample_2
    x <- length(sample_1)
  }
  return(p_value_final)
}

p_values_loop_one_p <- rep(NA, num_sim_v2) # creates a placeholder vector (where the output of the loop will be saved) with 10000 'NA' values

for (i in 1:num_sim_v2) {
   sample <- rnorm(30)
   p_value_v2 <- double.sample(sample)
   p_values_loop_one_p[i] <- p_value_v2
}

p_values_lt05_one_p <- p_values_loop_one_p < 0.05 # creates a vector containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

proportion_05_one_p <- sum(p_values_lt05_one_p == TRUE)/10000 # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(proportion_05_one_p) # prints the result
```

> I've also included code below that evaluates false positive rates that would be achieved if the researcher 1) doubles the sample every time their pvalue is not significant, but it's less than 0.5; 2) doubles the sample every time their pvalue is not significant, but it's less than 0.75, or 3) doubles their sample every time they get ANY pvalue that is not significant. I've included loops so that, in all three cases, the sample continues to be doubled until the specified conditions are met. (I should note that this code crashed R every time I tried to run it, so I did not evalute it in this writeup.)

```{r, eval = FALSE}
double_sample_many_ps <- function (p, x) {
  sample_1 <- rnorm(x)
  test_sample_1 <- t.test(sample_1)
  p_value_final <- test_sample_1$p.value
  while (p_value_final < p & p_value_final > .05){
    sample_2 <- append(sample_1, rnorm(x))
    test_sample_2 <- t.test(sample_2)
    p_value_final <- test_sample_2$p.value
    sample_1 <- sample_2
    x <- length(sample_1)
  }
  return(p_value_final)
}

x <- 30
p_values_many_ps <- replicate(10000, sapply(list, double_sample_loop_many_ps, x=x))

p_values_many_ps <- setNames(melt(p_values_many_ps), c("upper_p_value","iteration", "p_value"))

p_values_many_ps<- p_values_many_ps %>% 
  mutate(upper_p_value = as.factor(upper_p_value)) %>% 
  mutate(upper_p_value = ifelse(upper_p_value == '1', "p < .5", 
                            ifelse(upper_p_value == '2', "p < .75",
                                   ifelse(upper_p_value == '3', "p < 1", NA))))

p_values_many_ps <- p_values_many_ps %>% 
  mutate(significant = ifelse((p_value < .05), "TRUE", "FALSE")) # creates a column containing logical values of 'true' where the p-value is less than .05 and 'false' where the p-value is equal to or greater than .05

proportions_many_ps <- p_values_many_ps %>% 
  group_by(upper_p_value) %>% 
  summarise(proportion_true = sum(significant == "TRUE")/10000) # divides number of p-values that are less than .05 by the total number of p-values, which in this case is 10000

print(proportions_many_ps) # prints the result
```

